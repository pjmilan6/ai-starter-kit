{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Synthetic Data Generation\n",
    "\n",
    "This guide provides a quickstart for creating a synthetic QA and Retrieval-Augmented Generation (RAG) dataset using your own document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We recommend using Python 3.11. Also, make sure you have the necessary packages installed. If not, install them using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-sambanova==0.1.3\n",
    "!pip install \"unstructured[pdf,local-inference]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use [SambaNova Cloud](https://cloud.sambanova.ai) models, you'll need to set your API key. Run the following code to securely input your [SambaNova Cloud API Key](https://cloud.sambanova.ai/apis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "if not os.getenv(\"SAMBANOVA_API_KEY\"):\n",
    "    os.environ[\"SAMBANOVA_API_KEY\"] = getpass.getpass(\n",
    "        \"Enter your SambaNova Cloud API key: \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load data\n",
    "\n",
    "First, we will load the data from your source files using the [Unstructured](https://docs.unstructured.io/open-source/introduction/quick-start) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "def extract_pdf(file_path):\n",
    "    \"\"\"Extract text and tables from PDF file\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        List[Element]: A list of document elements (text, tables, etc.) extracted from the PDF.\n",
    "    \"\"\"\n",
    "    raw_pdf_elements = partition_pdf(\n",
    "        filename=file_path,\n",
    "        extract_images_in_pdf=False, \n",
    "        strategy='hi_res',\n",
    "        hi_res_model_name='yolox',\n",
    "        infer_table_structure=False,\n",
    "        chunking_strategy='by_title',\n",
    "        max_characters=4096,\n",
    "        combine_text_under_n_chars=500)\n",
    "\n",
    "    return raw_pdf_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will store the extracted elements into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "text_documents = []\n",
    "table_documents = []\n",
    "raw_pdf_elements = extract_pdf(\"./data/SambaNova_Dataflow.pdf\")\n",
    "for document in raw_pdf_elements:\n",
    "    if document.category == 'Table':\n",
    "        #transform table documents into langchain documents\n",
    "        table_documents.append(Document(page_content=document.metadata.text_as_html))\n",
    "    else:\n",
    "        text_documents.append(Document(page_content=document.text))\n",
    "\n",
    "print(len(table_documents))        \n",
    "print(len(text_documents))\n",
    "documents = text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate QA pairs\n",
    "\n",
    "With our granular documents ready, we can use a Large Language Model (LLM) to create QA pairs. Consider the following:\n",
    "\n",
    "- Depending on the dataset's purpose, you may want the model to include references used to generate the answer.\n",
    "- You might want the model to include reasoning steps from context to answer. A good strategy for this is [Chain of Thought (CoT)](https://www.promptingguide.ai/techniques/cot).\n",
    "- The model should generate a structured output from which we can extract the question, the thought process, the answer, and the references\n",
    "\n",
    "First, we'll initialize our LLM and define the schema for the QA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_sambanova import ChatSambaNovaCloud\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "class SyntheticDatum(BaseModel):\n",
    "    \"\"\"Model of a synthetic generated datum\"\"\"\n",
    "    question: str = Field(description='generated question')\n",
    "    answer: str = Field(description='generated answer')\n",
    "    references: list[str] = Field(description='references for generated answer')\n",
    "    thought: str = Field(description='thought for answer generation')\n",
    "\n",
    "\n",
    "class SyntheticData(BaseModel):\n",
    "    \"\"\"Model of a synthetic data generation\"\"\"\n",
    "    data: list[SyntheticDatum] = Field(description='synthetic data pairs')    \n",
    "    \n",
    "# Set the LLM to generate the QA pairs\n",
    "llm = ChatSambaNovaCloud(\n",
    "    model=\"Meta-Llama-3.1-8B-Instruct\",\n",
    "    temperature=0.01,\n",
    "    max_tokens=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a prompt instructing the model to generate QA pairs using the provided document and the specified number of QA pairs. The prompt will ask the model to generate a list of JSON objects containing the question, thought process, answer, and references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate([\n",
    "        (\"system\", \"You are a JSON generator who generates machine-readable JSON\"),\n",
    "        (\"human\", \"\"\"\n",
    "            Based on the following document, follow the instruction below\n",
    "            Document:\n",
    "            {document}\n",
    "            Instruction:\n",
    "            Generate {amount} of unique question, thought, answer, and references from the above document in the following JSON format. \n",
    "            The answers must avoid words that are not specific (e.g., \"many\", \"several\", \"few\", etc.). \n",
    "            The answers must contain specific, verbose, self-contained, grammatically correct sentences that answer the question comprehensively. \n",
    "            The answers must strictly contain content from the document and no content from outside the document. \n",
    "            There may be multiple references that contain verbatim text from the document to support the answers.             \n",
    "            JSON format:\n",
    "            [\n",
    "                {{\n",
    "                    \"question\": \"<generated question>\",            \n",
    "                    \"thought\": \"<generated thought on what is needed to answer the question. Start with 'To answer the question, I need'>\",\n",
    "                    \"answer\": \"<generated answer>\",\n",
    "                    \"references\": [\n",
    "                        \"<verbatim text from document that supports the answer>\",\n",
    "                        \"<verbatim text from document that supports the answer>\"\n",
    "                    ]\n",
    "                }}\n",
    "            ]\n",
    "            The first character of the response must be '[' and the last character must be ']'. No header text should be included.\n",
    "            \"\"\"\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the prompt defined, we can create a method to instantiate a LangChain chain, pass the input arguments (the context document and the number of QA pairs to generate), and process the model's response using the defined QA data schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs(context, amount, include_context = False, include_thoughts = False, include_references = False):\n",
    "    \"\"\"Generate synthetic QA pairs from a given context using a LangChain chain.\n",
    "\n",
    "    Args:\n",
    "        context (str): The source text to generate questions and answers from.\n",
    "        amount (int): Number of QA pairs to generate.\n",
    "        include_context (bool): Whether to include the original context in each output entry.\n",
    "        include_thoughts (bool): Whether to include model 'thoughts' in each QA pair.\n",
    "        include_references (bool): Whether to include reference sources in each QA pair.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of dictionaries containing QA pairs (and optional metadata).\n",
    "    \"\"\"\n",
    "\n",
    "    synthetic_datum_parser = JsonOutputParser(pydantic_object=SyntheticData)\n",
    "    qa_generate_chain = prompt | llm | synthetic_datum_parser\n",
    "    qa_pairs = []\n",
    "    generation = qa_generate_chain.invoke({'document': context, 'amount': amount})\n",
    "    for datum in generation:\n",
    "        qa_pair = {\n",
    "            'question': datum['question'],\n",
    "            'context': context if include_context else None,\n",
    "            'answer': datum['answer'],\n",
    "            'thought': datum['thought'] if include_thoughts else None,\n",
    "            'references': datum['references'] if include_references else None,\n",
    "        }\n",
    "        qa_pair = {k: v for k, v in qa_pair.items() if v is not None}\n",
    "        qa_pairs.append(qa_pair)\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example where we create a series of synthetic data pairs, including the original context (useful for training models for Retrieval-Augmented Generation (RAG) applications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What are the three living species of elephants currently recognised?',\n",
       "  'answer': 'The three living species of elephants currently recognised are the African bush elephant, the African forest elephant, and the Asian elephant.'},\n",
       " {'question': 'What is the family of the African and Asian elephants?',\n",
       "  'answer': 'The African and Asian elephants are the only surviving members of the family Elephantidae.'},\n",
       " {'question': 'What are the extinct relatives of elephants?',\n",
       "  'answer': 'The extinct relatives of elephants include mammoths and mastodons.'},\n",
       " {'question': 'What is the order of the African and Asian elephants?',\n",
       "  'answer': 'The African and Asian elephants are members of the order Proboscidea.'},\n",
       " {'question': 'What are the largest living land animals?',\n",
       "  'answer': 'Elephants are the largest living land animals.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_doc=\"\"\"Elephants are the largest living land animals. \n",
    "Three living species are currently recognised:\n",
    "the African bush elephant (Loxodonta africana),\n",
    "the African forest elephant (L. cyclotis), and the Asian elephant (Elephas maximus). \n",
    "They are the only surviving members of the family Elephantidae and the order Proboscidea;\n",
    "extinct relatives include mammoths and mastodons.\"\"\"\n",
    "\n",
    "generate_qa_pairs(sample_doc, 5, include_context = False, include_thoughts = False, include_references = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate full dataset\n",
    "\n",
    "We will create a simple method to convert each QA pair dictionary into a single string with the format required for the fine-tuning process. Then, we will iterate over each chunk of our source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_pairs_to_prompt_completion(qa_pairs):\n",
    "    \"\"\"Converts QA pair dictionaries into prompt-completion strings formatted for fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (Union[dict, List[dict]]): A single QA pair or a list of QA pairs.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of JSON-formatted strings, each representing a prompt-completion example.\n",
    "    \"\"\"\n",
    "    # Ensure input is a list of QA pairs\n",
    "    if isinstance(qa_pairs, dict):\n",
    "        qa_pairs = [qa_pairs]\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    for pair in qa_pairs:\n",
    "        #line = {'prompt': f'{\"You are a helpful assistant for question-answering tasks.\"}{pair[\"question\"]}', 'completion': ''}\n",
    "        line = {'prompt': f'{pair[\"question\"]}', 'completion': ''}\n",
    "\n",
    "        # Optionally include context if available \n",
    "        if pair.get('context'):\n",
    "            line['prompt'] += f'\\nContext: {pair[\"context\"]}\\n'\n",
    "        \n",
    "        # Optionally include the model's \"thoughts\" before the answer\n",
    "        if pair.get('thought'):\n",
    "            line['completion'] += f'Thought: {pair[\"thought\"]}\\n'\n",
    "        \n",
    "        # Append the answer directly to the completion\n",
    "        line['completion'] += f'Answer: {pair[\"answer\"]}\\n'\n",
    "\n",
    "        # Optionally include references at the end\n",
    "        if pair.get('references'):\n",
    "            line['completion'] += f'References: {pair[\"references\"]}\\n'\n",
    "        \n",
    "        # Convert the prompt-completion pair to a JSONL line\n",
    "        lines.append(json.dumps(line))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating Q&A pairs for document: Multi-Tenant and Concurrent Applications on RDA\n",
      "\n",
      "While DataScale systems can be used for large-scale applications as described earlier, they can also support multiple concurrent applications and provide multi-tenant isolation as shown in Figure 7. Teams or applications may only require a portion of a system, and organizations can use the multi-tenant functionality to provide machine-learning, private-cloud resources that serve multiple departments or customers. It is also possible to dedicate some portion of the RDU to training updated models while other portions execute previously trained models for inferencing and results generation.\n",
      "\n",
      "High Performance Concurrent Secure Mixed Workloads Application Isolation Multi-Tenancy Training Post- Processing\n",
      "\n",
      "Figure 7 - Supporting multiple users or workloads simultaneously\n",
      "'NoneType' object is not iterable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['{\"prompt\": \"What are the performance and efficiency challenges of traditional instruction set architectures?\", \"completion\": \"Answer: The performance and efficiency challenges of traditional instruction set architectures have become apparent due to the rapid expansion of applications that can be characterized by dataflow processing, such as natural-language processing and recommendation engines.\\\\n\"}',\n",
       " '{\"prompt\": \"Why have the sizable, generation-to-generation performance gains for multi-core processors tapered off?\", \"completion\": \"Answer: The sizable, generation-to-generation performance gains for multi-core processors have tapered off because developers can no longer depend on traditional performance improvements to power more complex and sophisticated applications.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the explosion in the use of deep learning causing?\", \"completion\": \"Answer: The explosion in the use of deep learning is amplifying the gap between required and available computing, as the compute power used for notable artificial intelligence achievements has doubled every 3.4 months between 2012 and 2020.\\\\n\"}',\n",
       " '{\"prompt\": \"Why is there a need for learning systems that unify machine-learning training and inference?\", \"completion\": \"Answer: There is a need for learning systems that unify machine-learning training and inference because many real-life systems demonstrate continual and sometimes unpredictable change, which means predictive accuracy of models declines without frequent updates.\\\\n\"}',\n",
       " '{\"prompt\": \"What other workloads will require acceleration besides machine learning?\", \"completion\": \"Answer: Other workloads such as analytics, scientific applications, and even SQL data processing all exhibit dataflow characteristics and will require acceleration.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the key attribute that enables highly efficient dataflow processing?\", \"completion\": \"Answer: Native dataflow is the key attribute that enables highly efficient dataflow processing by describing commonly occurring operators in machine-learning frameworks and DSLs in terms of parallel patterns that capture parallelizable computation on both dense and sparse data collections along with corresponding memory access patterns.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the trend in deep-learning model development that requires larger model sizes?\", \"completion\": \"Answer: A key trend in deep-learning model development uses increasingly large model sizes to gain higher accuracy and deliver more sophisticated functionality, such as leveraging billions of data-points to enable more accurate Natural Language Generation.\\\\n\"}',\n",
       " '{\"prompt\": \"What type of data structures involve large sparse data structures that consist of mostly zero values?\", \"completion\": \"Answer: Recommender systems, friend-of-friends problems, knowledge graphs, some life-science domains, and more involve large sparse data structures that consist of mostly zero values.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the goal of a new architecture in terms of model mapping?\", \"completion\": \"Answer: A new architecture should automatically enable scaling across infrastructure without added development and orchestration complexity and avoid the need for model developers to become experts in system architecture and parallel computing.\\\\n\"}',\n",
       " '{\"prompt\": \"What type of processing tasks should a new architecture allow?\", \"completion\": \"Answer: A new architecture should allow the unification of pre-processing and post-processing of data on a single platform, as deep learning models grow and incorporate a wider variety of data types.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the SambaNova Reconfigurable Dataflow Architecture?\", \"completion\": \"Answer: The SambaNova Reconfigurable Dataflow Architecture is a computing architecture designed to enable the next generation of machine learning and high performance computing applications.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the main advantage of the Reconfigurable Dataflow Architecture?\", \"completion\": \"Answer: The Reconfigurable Dataflow Architecture provides a flexible, dataflow execution model that pipelines operations, enables programmable data access patterns and minimizes excess data movement found in fixed, core-based, instruction set architectures.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the SambaNova Reconfigurable Dataflow Unit?\", \"completion\": \"Answer: The SambaNova Reconfigurable Dataflow Unit is a next-generation processor designed to provide native dataflow processing and programmable acceleration.\\\\n\"}',\n",
       " '{\"prompt\": \"What is SambaFlow?\", \"completion\": \"Answer: SambaFlow is a complete software stack designed to take input from standard machine-learning frameworks such as PyTorch and TensorFlow.\\\\n\"}',\n",
       " '{\"prompt\": \"What is SambaNova Systems DataScale?\", \"completion\": \"Answer: SambaNova Systems DataScale is a complete, rack-level, data-center-ready accelerated computing system.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the limitation of traditional core-based architectures in managing data flow?\", \"completion\": \"Answer: Traditional core-based architectures are limited in managing data flow because they primarily rely on hardware to manage communications, which are mainly limited to cache and memory transfers.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the alternative approach provided by the SambaNova Reconfigurable Dataflow Architecture?\", \"completion\": \"Answer: The SambaNova Reconfigurable Dataflow Architecture provides an alternative approach where the communications can be programmed and optimized to best suit how data should transit a series of computations.\\\\n\"}',\n",
       " '{\"prompt\": \"What is spatial programming, and how does it relate to SambaFlow?\", \"completion\": \"Answer: Spatial programming involves configuring the physical resources of the RDU so that data progresses efficiently in parallel across the fabric of the chip, and SambaFlow uses spatial programming to program the sequence of instructions (layers) running on the chip at a specific time.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the impact of implementing a complex compute graph on core-based architecture compared to the SambaNova Reconfigurable Dataflow Architecture?\", \"completion\": \"Answer: Implementing a complex compute graph on core-based architecture requires executing a large number of sequential instructions, where there is no optimization of dataflow for a particular workload, whereas the SambaNova Reconfigurable Dataflow Architecture achieves much higher throughput, higher hardware utilization, and lower latency.\\\\n\"}',\n",
       " '{\"prompt\": \"What are some examples of parallel patterns that can be optimized using the SambaNova Reconfigurable Dataflow Architecture?\", \"completion\": \"Answer: Some examples of parallel patterns that can be optimized using the SambaNova Reconfigurable Dataflow Architecture include Map, Zip, and Reduce, which involve different types of data flow and can be optimized for specific workloads.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the primary issue with traditional hardware architectures for dataflow-oriented workloads?\", \"completion\": \"Answer: Traditional hardware architectures operate on a stream of low-level instructions that have poor energy efficiency and force a kernel-by-kernel programming model, leading to excess data and instruction movement, which results in poor processor utilization.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the main advantage of the SambaNova Reconfigurable Dataflow Architecture?\", \"completion\": \"Answer: The SambaNova Reconfigurable Dataflow Architecture creates custom processing pipelines that allow data to flow through the complete computation graph, using a spatial programming model to optimize compute layout and minimize data movement to achieve high hardware utilization.\\\\n\"}',\n",
       " '{\"prompt\": \"How does the SambaFlow perform a one-time configuration to map the entire model onto RDUs?\", \"completion\": \"Answer: When an application is launched, SambaFlow performs a one-time configuration to map the entire model onto RDUs, allowing the entire system to perform as a pipeline with different parts of the RDUs executing different layers of a model, working simultaneously with different data at each stage.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the primary benefit of the spatial programming techniques applied in the SambaNova Reconfigurable Dataflow Architecture?\", \"completion\": \"Answer: Spatial programming techniques are applied to ensure that the layout of the operations on the RDU minimizes data movement to achieve high efficiency.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the main difference between the execution of the convolution graph on a traditional core-based architecture and the SambaNova Reconfigurable Dataflow Architecture?\", \"completion\": \"Answer: During execution, each kernel must be loaded onto the CPU or GPU, data and weights are read from memory, calculations are performed and the output results are written to memory in traditional core-based architectures, whereas in the SambaNova Reconfigurable Dataflow Architecture, data flows through the complete computation graph, minimizing data movement and achieving high hardware utilization.\\\\n\"}',\n",
       " '{\"prompt\": \"What are the key advantages of the RDU over fixed ASIC designs?\", \"completion\": \"Answer: The key advantages of the RDU over fixed ASIC designs include the ability to be optimized and configured for various workloads, rapid reconfiguration, and the ability to adapt to the latest algorithm breakthroughs.\\\\n\"}',\n",
       " '{\"prompt\": \"How does the reconfiguration time of the RDU compare to FPGAs?\", \"completion\": \"Answer: The RDU can be reconfigured in microseconds, which is a significant improvement over the time-consuming, complex, low-level programming and long compilation times of FPGAs.\\\\n\"}',\n",
       " '{\"prompt\": \"What benefits do programmers gain from using the RDU?\", \"completion\": \"Answer: Programmers gain the ability to work in high-level DSLs while providing enhanced execution efficiency, simplified compilation, and performance.\\\\n\"}',\n",
       " '{\"prompt\": \"How does the RDU\\'s flexibility and reconfigurability compare to FPGAs?\", \"completion\": \"Answer: The RDU offers a higher level of flexibility and reconfigurability compared to FPGAs, with the ability to be reconfigured in microseconds, whereas FPGAs require time-consuming, complex, low-level programming and long compilation times.\\\\n\"}',\n",
       " '{\"prompt\": \"What are the advantages of the RDU\\'s rapid reconfiguration?\", \"completion\": \"Answer: The RDU\\'s rapid reconfiguration enables the architecture to be quickly repurposed for new needs or to adapt to the latest algorithm breakthroughs.\\\\n\"}',\n",
       " '{\"prompt\": \"What benefits does the dataflow approach offer in terms of memory usage?\", \"completion\": \"Answer: The dataflow approach reduces memory bandwidth needs and enables the use of much larger, terabyte-sized attached memory for large model support.\\\\n\"}',\n",
       " '{\"prompt\": \"How does the dataflow approach handle large batch sizes?\", \"completion\": \"Answer: The simultaneous processing of an entire graph in a pipelined fashion enables high utilization across a broad range of batch sizes and eliminates the requirement to use large batch sizes to achieve acceptable efficiency.\\\\n\"}',\n",
       " '{\"prompt\": \"What features of the dataflow architecture enable high-performance model execution?\", \"completion\": \"Answer: The dataflow architecture\\'s high on-chip memory capacity and localization, as well as high internal fabric bandwidth, enable the ability to run very large models at high performance.\\\\n\"}',\n",
       " '{\"prompt\": \"What type of performance does the pipeline processing on RDUs provide?\", \"completion\": \"Answer: Pipeline processing on RDUs provides predictable, low-latency performance.\\\\n\"}',\n",
       " '{\"prompt\": \"How does the hierarchy of the dataflow architecture simplify compiler mapping?\", \"completion\": \"Answer: The hierarchy of this architecture simplifies compiler mapping and significantly improves execution efficiency.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the primary function of the SambaNova Cardinal SN10 Reconfigurable Dataflow Unit?\", \"completion\": \"Answer: The primary function of the SambaNova Cardinal SN10 Reconfigurable Dataflow Unit is to efficiently execute dataflow graphs.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the structure of the Reconfigurable Dataflow Unit (RDU)?\", \"completion\": \"Answer: The RDU consists of a tiled array of reconfigurable processing and memory units connected through a high-speed, three-dimensional on-chip switching fabric.\\\\n\"}',\n",
       " '{\"prompt\": \"How is the RDU configured for application execution?\", \"completion\": \"Answer: When an application is started, SambaFlow configures the RDU elements to execute an optimized dataflow graph for that specific application.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the role of SambaFlow in the RDU configuration process?\", \"completion\": \"Answer: SambaFlow configures the RDU elements to execute an optimized dataflow graph for a specific application.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the significance of the three-dimensional on-chip switching fabric in the RDU?\", \"completion\": \"Answer: The high-speed, three-dimensional on-chip switching fabric connects the reconfigurable processing and memory units in the RDU, enabling efficient dataflow execution.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the primary function of the Pattern Compute Unit (PCU) in the RDU architecture?\", \"completion\": \"Answer: The primary function of the Pattern Compute Unit (PCU) is to execute a single, innermost-parallel operation in an application.\\\\n\"}',\n",
       " '{\"prompt\": \"How does the Pattern Memory Unit (PMU) contribute to the RDU architecture?\", \"completion\": \"Answer: The Pattern Memory Unit (PMU) provides on-chip memory capacity and performs a number of specialized intelligent functions, minimizing data movement, reducing latency, increasing bandwidth, and avoiding off-chip memory accesses.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the purpose of the switching fabric in the RDU architecture?\", \"completion\": \"Answer: The switching fabric is a high-speed network that connects PCUs and PMUs, composed of three switching networks: scalar, vector, and control, which differ in granularity of data being transferred.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the function of Address Generator Units (AGUs) and Coalescing Units (CUs) in the RDU architecture?\", \"completion\": \"Answer: AGUs and CUs provide the interconnect between RDUs and the rest of the system, including off-chip DRAM, other RDUs, and the host processor, enabling efficient processing of sparse and graph-based datasets.\\\\n\"}',\n",
       " '{\"prompt\": \"What are the performance advantages of the RDU architecture compared to traditional architectures?\", \"completion\": \"Answer: The RDU architecture offers significant performance advantages over traditional architectures due to reconfigurability, exploitation of parallelism at multiple levels, and the elimination of instruction processing overhead.\\\\n\"}',\n",
       " '{\"prompt\": \"What is SambaFlow designed to do?\", \"completion\": \"Answer: SambaFlow is designed to be an easy-to-use way to shield algorithm developers from low-level tuning needs that are common on other architectures.\\\\n\"}',\n",
       " '{\"prompt\": \"How does SambaFlow connect to machine-learning frameworks?\", \"completion\": \"Answer: SambaFlow connects to machine-learning frameworks and analyzes models to build dataflow graphs.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the advantage of using SambaFlow for workload scaling?\", \"completion\": \"Answer: SambaFlow automates the scaling of workloads across multiple RDUs, avoiding the need for developing external frameworks or using trial-and-error guesswork to split the model apart to achieve optimal results.\\\\n\"}',\n",
       " '{\"prompt\": \"How long does RDU reconfiguration take?\", \"completion\": \"Answer: RDU reconfiguration is lightweight and can take 10-40 microseconds depending on model complexity.\\\\n\"}',\n",
       " '{\"prompt\": \"What components optimize the application and manage system resources in SambaFlow?\", \"completion\": \"Answer: SambaFlow has several components that optimize the application and manage system resources.\\\\n\"}',\n",
       " '{\"prompt\": \"What are the popular open-source machine-learning frameworks supported by SambaFlow?\", \"completion\": \"Answer: SambaFlow supports the common open-source machine-learning frameworks, PyTorch and TensorFlow.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the purpose of the Dataflow Graph Analyzer in SambaFlow?\", \"completion\": \"Answer: The Dataflow Graph Analyzer accepts models from the frameworks, analyzes the model to extract the dataflow graph, determines the computation and communication requirements for each operator, and allocates the appropriate RDU resources.\\\\n\"}',\n",
       " '{\"prompt\": \"How does SambaFlow optimize model compilation and execution?\", \"completion\": \"Answer: SambaFlow uses a push-button model compilation and optimization approach, which allows high performance to be obtained out of the box without the need for hand tuning, and also performs high-level transformations like meta-pipelining, multi-section support, and parallelization.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the advantage of using SambaFlow\\'s spatial programming model?\", \"completion\": \"Answer: The spatial programming model automatically assigns resources in the most efficient manner to provide a complete processing pipeline that enables high RDU utilization by minimizing data movement and off-chip accesses.\\\\n\"}',\n",
       " '{\"prompt\": \"What are the supported protocols for high-performance data transfer in SambaFlow?\", \"completion\": \"Answer: SambaFlow supports high-performance data transfer with protocol support for RoCE, RDMA, Ethernet, and Infiniband protocols.\\\\n\"}',\n",
       " '{\"prompt\": \"What are the drivers behind the trend towards large memory capacities in various applications?\", \"completion\": \"Answer: The drivers behind the trend towards large memory capacities in various applications include the need for a very large number of parameters in NLP to enable the summarization of complex text passages or generation of improved text suggestions, and the requirement for rich user and catalog embeddings in recommender systems to achieve higher recommendation accuracy and conversion rates.\\\\n\"}',\n",
       " '{\"prompt\": \"How do large models require large memory capacities?\", \"completion\": \"Answer: Large models require large memory capacities because they are often partitioned into small parts, each of which fits into GPU memory, and the latest large NLP models are often trained on configurations built with thousands of GPUs.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the impact of large memory capacities on innovation in the field of large models?\", \"completion\": \"Answer: Sheer size, cost, and complexity serve to limit innovation to large, well-funded organizations due to the requirement for large memory capacities in large models.\\\\n\"}',\n",
       " '{\"prompt\": \"How does the SambaNova Datascale System enable the use of large models?\", \"completion\": \"Answer: The SambaNova Datascale System enables the use of large models by utilizing the dataflow processing model and large on-chip capacity, which reduces off-chip communication and pressure on memory bandwidth, and allows the use of terabyte-sized attached memory.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the benefit of running models across multiple RDUs in a system?\", \"completion\": \"Answer: Running models across multiple RDUs in a system is simplified through automated data and model parallel scaling.\\\\n\"}',\n",
       " '{\"prompt\": \"What capabilities of the RDA can simplify and facilitate additional parts of the machine-learning life cycle?\", \"completion\": \"Answer: The capabilities of the RDA that can simplify and facilitate additional parts of the machine-learning life cycle include the integration of pre- and post-processing to avoid additional ETL overhead, reconfigurability that facilitates rapid iteration on model development by experimentation and adjustments to models in the development and testing phase, and support for both high throughput, large batch-size training as well as small batch-size inference.\\\\n\"}',\n",
       " '{\"prompt\": \"How does the integration of pre- and post-processing in RDUs avoid additional ETL overhead?\", \"completion\": \"Answer: The integration of pre- and post-processing in RDUs avoids additional ETL overhead by enabling the processing of data before and after it is used for training or inference, thereby reducing the need for additional data transformation and loading steps.\\\\n\"}',\n",
       " '{\"prompt\": \"What types of models can be developed using the continuous learning or incremental training mode?\", \"completion\": \"Answer: The continuous learning or incremental training mode allows for the development of new and interesting types of models that combine training and inference in a continuous process, enabling the creation of models that would not be possible in conventional life cycle flows.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the purpose of the Data Lake in the machine-learning life cycle?\", \"completion\": \"Answer: The Data Lake is a repository for raw, unprocessed data that can be used for training and testing machine-learning models, enabling the creation of models that are based on real-world data.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the role of OLTP & DW in the machine-learning life cycle?\", \"completion\": \"Answer: OLTP & DW (Online Transactional Processing and Data Warehouse) are used to store and manage data that is used for training and testing machine-learning models, enabling the creation of models that are based on real-world data.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the unique feature of SambaNova DataScale systems and SambaFlow?\", \"completion\": \"Answer: SambaNova DataScale systems and SambaFlow provide a vertically integrated platform that is optimized from the algorithm, through the compiler, and down to the silicon.\\\\n\"}',\n",
       " '{\"prompt\": \"What type of applications can be accelerated using SambaNova DataScale systems and SambaFlow?\", \"completion\": \"Answer: SambaNova DataScale systems and SambaFlow can accelerate machine-learning and HPC applications.\\\\n\"}',\n",
       " '{\"prompt\": \"How can one learn more about accelerating machine-learning and HPC applications using SambaNova DataScale systems and SambaFlow?\", \"completion\": \"Answer: One can visit sambanova.ai or call +1 650 263-1153 to speak to a SambaNova representative.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the purpose of SambaNova DataScale systems and SambaFlow?\", \"completion\": \"Answer: The purpose of SambaNova DataScale systems and SambaFlow is to provide a unique vertically integrated platform that is optimized from the algorithm, through the compiler, and down to the silicon to achieve breakthrough performance and flexibility for innovators creating the next generation of machine-learning and deep-learning applications.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the next generation of applications that SambaNova DataScale systems and SambaFlow are designed to support?\", \"completion\": \"Answer: SambaNova DataScale systems and SambaFlow are designed to support the next generation of machine-learning and deep-learning applications.\\\\n\"}',\n",
       " '{\"prompt\": \"Who owns the rights to SambaNova Systems?\", \"completion\": \"Answer: SambaNova Systems, Inc. owns the rights to SambaNova Systems.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the SambaNova logo and other product names are registered as?\", \"completion\": \"Answer: The SambaNova logo and all product and service names mentioned herein are registered trademarks or trademarks of SambaNova Systems in the United States and other countries.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the version of the document?\", \"completion\": \"Answer: The version of the document is 1.0.\\\\n\"}',\n",
       " '{\"prompt\": \"Who are the trademark holders of SambaNova Systems?\", \"completion\": \"Answer: SambaNova Systems holds the trademarks of SambaNova Systems in the United States and other countries.\\\\n\"}',\n",
       " '{\"prompt\": \"What is the purpose of mentioning other brand names in the document?\", \"completion\": \"Answer: The other brand names mentioned herein are for identification purposes only and may be the trademarks of their respective holder(s).\\\\n\"}']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = []\n",
    "for document in documents:\n",
    "    try: \n",
    "        qa_pairs = generate_qa_pairs(\n",
    "            context=document.page_content,\n",
    "            amount=5,\n",
    "            include_context=False,\n",
    "            include_thoughts=False,\n",
    "            include_references=False,\n",
    "        )\n",
    "        lines.extend(qa_pairs_to_prompt_completion(qa_pairs))\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating Q&A pairs for document: {document.page_content}\")\n",
    "        print(e)\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the list of JSON strings into a jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.jsonl\", \"w\") as f:\n",
    "    for line in lines:\n",
    "        json_obj = json.loads(line)  # ensure it's valid JSON\n",
    "        f.write(json.dumps(json_obj) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3_aramco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
